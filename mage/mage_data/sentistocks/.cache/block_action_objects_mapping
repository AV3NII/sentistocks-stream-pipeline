{"block_file": {"data_exporters/export_coinbase_histo.py:data_exporter:python:export coinbase histo": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nimport os\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    try:\n        project_id = os.environ['GCP_PROJECT_ID']\n        # Define table ID and configuration settings\n        table_id = f'{project_id}.coinbase.histo'\n        config_path = os.path.join(get_repo_path(), 'io_config.yaml')\n        config_profile = 'default'\n\n        # Export DataFrame to BigQuery\n        BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n            df,\n            table_id,\n            if_exists='replace',  # Specify resolution policy if table name already exists\n        )\n    except Exception as e:\n        print(f\"Error occurred during data export to BigQuery: {e}\")\n        raise ValueError(\"Failed to export data to BigQuery.\")\n", "file_path": "data_exporters/export_coinbase_histo.py", "language": "python", "type": "data_exporter", "uuid": "export_coinbase_histo"}, "data_loaders/load_bigquery_state.py:data_loader:python:load bigquery state": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nimport os\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_big_query(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n\n    \"\"\"\n    project_id = os.environ['GCP_PROJECT_ID']\n    query = f\"SELECT MAX(Date) AS Date FROM `{project_id}.coinbase.histo` limit 1\"\n    config_path = os.path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    return BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).load(query)\n    \n\n\n", "file_path": "data_loaders/load_bigquery_state.py", "language": "python", "type": "data_loader", "uuid": "load_bigquery_state"}, "data_loaders/load_coinbase_histodata.py:data_loader:python:load coinbase histodata": {"content": "import io\nimport os\nimport pandas as pd\nimport requests\nfrom datetime import datetime, timedelta\nimport time\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    session = requests.Session()\n    session.headers.update({\n        'Content-Type': 'application/json',\n        'User-Agent': 'Python http.client'\n    })\n\n    def get_json_response(url):\n        try:\n            response = session.get(url)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            print(f\"Request failed: {e}\")\n            return None\n\n    def get_data(token, start_date, end_date):\n        url = f'https://api.exchange.coinbase.com/products/{token}-EUR/candles?start={start_date}&end={end_date}&granularity=86400'\n        return get_json_response(url)\n\n    def process_response(data):\n        if not data or not isinstance(data, list) or len(data[0]) != 6:\n            return pd.DataFrame()\n        try:\n            df = pd.DataFrame(data, columns=['date', 'low', 'high', 'open', 'close', 'volume'])\n            df['date'] = pd.to_datetime(df['date'], unit='s')\n            return df\n        except Exception as e:\n            print(f\"Error processing data: {e}\")\n            return pd.DataFrame()\n\n\n    def fetch_all_tokens():\n        url = \"https://api.exchange.coinbase.com/products\"\n        products = get_json_response(url)\n        return [product['id'].split('-')[0] for product in products if '-' in product['id'] and product['id'].split('-')[1] == 'EUR'] if products else []\n\n    \n    end_date = kwargs['execution_date'].date()\n    start_date = datetime.strptime(kwargs['start_date'], '%Y-%m-%d')\n    #earliest_date = datetime.strptime('2023-7-1', '%Y-%m-%d')\n\n    # Check if the start_date is before the earliest accessible date\n    #if start_date < earliest_date:\n    #    print(f\"Adjusted Start Date from {start_date} to {earliest_date}\")\n    #    start_date = earliest_date\n\n\n    print(\"Start Date:\", start_date)\n    print(\"End Date:\", end_date)\n\n    all_tokens = fetch_all_tokens()\n    wanted_tokens = {'AAVE', 'USDT', 'CHZ', 'SOL', 'SHIB', 'APE',\n                     'ADA', 'CRO', 'DOGE', 'CRV', 'AXS', 'ETC', 'UNI',\n                     'SNX', 'AVAX', 'ETH', 'BAT', 'LTC', 'MASK', 'ATOM',\n                     'EOS', 'BICO', 'BTC', 'DOT', 'TRX'}\n    selected_tokens = set(all_tokens).intersection(wanted_tokens)\n\n    master_df = pd.DataFrame()\n    for token in selected_tokens:\n        candles = get_data(token, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))\n        token_df = process_response(candles)\n        if not token_df.empty:\n            token_df['token'] = token\n            master_df = pd.concat([master_df, token_df], ignore_index=True)\n            time.sleep(1)  # Be nice to the API\n\n    return master_df\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_coinbase_histodata.py", "language": "python", "type": "data_loader", "uuid": "load_coinbase_histodata"}, "transformers/trigger_pipline.py:transformer:python:trigger pipline": {"content": "import json\nimport requests\nimport pandas as pd\nfrom mage_ai.orchestration.triggers.api import trigger_pipeline\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n    \n@transformer\ndef transform(data: pd.DataFrame, *args, **kwargs):\n    if pd.isna(data['Date'].iloc[0]):\n        trigger_pipeline(\n            'coinbase_histo',\n            variables={ \"start_date\": \"2023-7-1\" },\n            check_status=False,\n            error_on_failure=False,\n            poll_interval=60,\n            poll_timeout=None,\n            schedule_name=None,  # Enter a unique name to create a new trigger each time\n            verbose=True,\n        )\n        \n    else:\n        print(f\"Most recent date in BigQuery is {data['Date'].iloc[0] }. Load full historical data\")\n        \n        trigger_pipeline(\n            'coinbase_histo',\n            variables={ \"start_date\":  str(data['Date'].iloc[0])},\n            check_status=False,\n            error_on_failure=False,\n            poll_interval=60,\n            poll_timeout=None,\n            schedule_name=None,  # Enter a unique name to create a new trigger each time\n            verbose=True,\n        )\n", "file_path": "transformers/trigger_pipline.py", "language": "python", "type": "transformer", "uuid": "trigger_pipline"}, "transformers/tranform_histo_datatypes.py:transformer:python:tranform histo datatypes": {"content": "from pyspark.sql import SparkSession, DataFrame\nfrom pyspark.sql.types import StructType, StructField, DateType, FloatType, StringType\nfrom pyspark.sql.functions import hour, month, year, col\nfrom pyspark.context import SparkContext\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    try:\n        \n        # Initialize Spark session\n        spark = SparkSession.builder \\\n            .appName('KafkaStreamingSparkTransform') \\\n            .master('local[*]') \\\n            .config(\"spark.jars\", \"/opt/spark/jars/*.jar\") \\\n            .config(\"spark.jars.packages\", \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.5\") \\\n            .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n            .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n            .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\") \\\n            .config(\"spark.hadoop.fs.gs.auth.service.account.json.keyfile\", \"/home/src/gcp-credentials.json\") \\\n            .getOrCreate()\n            #.master('spark://spark-master:7077') \\\n        \n        \n\n        kwargs['spark'] = spark\n    \n    except Exception as e:\n        # Log error message\n        print(f\"Error occurred during session building: {e}\")\n        # Raise the exception to halt the pipeline execution\n        raise e\n\n    # Specify your transformation logic here\n    schema = StructType([\n        StructField(\"date\", DateType(), True),\n        StructField(\"low\", FloatType(), True),\n        StructField(\"high\", FloatType(), True),\n        StructField(\"open\", FloatType(), True),\n        StructField(\"close\", FloatType(), True),\n        StructField(\"volume\", FloatType(), True),\n        StructField(\"token\", StringType(), True)\n    ])\n        \n    # Convert Pandas DataFrame to Spark DataFrame\n    df = spark.createDataFrame(data, schema)\n\n    # Rename columns\n    df = df.withColumnRenamed(\"date\", \"Date\") \\\n         .withColumnRenamed(\"low\", \"Lowest_Price\") \\\n         .withColumnRenamed(\"high\", \"Highest_Price\") \\\n         .withColumnRenamed(\"open\", \"Opening_Price\") \\\n         .withColumnRenamed(\"close\", \"Closing_Price\") \\\n         .withColumnRenamed(\"volume\", \"Volume_Traded\") \\\n         .withColumnRenamed(\"token\", \"Cryptocurrency\") \\\n    \n    df = df.withColumn(\"Month\", month(col(\"Date\")))\n    df = df.withColumn(\"Year\", year(col(\"Date\")))\n    return df.toPandas()\n\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "transformers/tranform_histo_datatypes.py", "language": "python", "type": "transformer", "uuid": "tranform_histo_datatypes"}, "transformers/transform_token_to_name.py:transformer:python:transform token to name": {"content": "from pyspark.sql import DataFrame\nfrom pyspark.sql.functions import when, col, month\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@transformer\ndef transform(data: DataFrame, *args, **kwargs):\n    try:\n\n        # Initialize or retrieve Spark session from kwargs\n        spark = kwargs.get('spark')\n        if spark is None:\n            from pyspark.sql import SparkSession\n            spark = SparkSession.builder \\\n                .appName('DataTransformation') \\\n                .getOrCreate()\n            print(\"New Spark session created.\")\n        else:\n            print(\"Using existing Spark session.\")\n\n        # Define the token mapping\n        token_mapping = {\n            'AAVE': 'Aave', 'USDT': 'Tether', 'CHZ': 'Chiliz', 'SOL': 'Solana', 'SHIB': 'Shiba Inu',\n            'APE': 'ApeCoin', 'ADA': 'Cardano', 'CRO': 'Crypto.com Coin', 'DOGE': 'Dogecoin',\n            'CRV': 'Curve DAO Token', 'AXS': 'Axie Infinity', 'ETC': 'Ethereum Classic',\n            'UNI': 'Uniswap', 'SNX': 'Synthetix', 'AVAX': 'Avalanche', 'ETH': 'Ethereum',\n            'BAT': 'Basic Attention Token', 'LTC': 'Litecoin', 'MASK': 'Mask Network',\n            'ATOM': 'Cosmos', 'EOS': 'EOS', 'BICO': 'Biconomy', 'BTC': 'Bitcoin', 'DOT': 'Polkadot'\n        }\n\n        # Apply the mapping using a series of when() statements\n        mapping_expr = col(\"Cryptocurrency\")\n        for token, name in token_mapping.items():\n            mapping_expr = when(col(\"Cryptocurrency\") == token, name).otherwise(mapping_expr)\n\n        # Apply the transformation\n        transformed_df = data.withColumn(\"Currency\", mapping_expr)\n\n        # Drop the old 'Cryptocurrency' column\n        if \"Cryptocurrency\" in transformed_df.columns:\n            transformed_df = transformed_df.drop(\"Cryptocurrency\")\n        print(transformed_df.columns)\n        \n        return transformed_df.toPandas()\n\n    except Exception as e:\n        print(f\"Error occurred during transformation: {e}\")\n        raise e\n\n\n", "file_path": "transformers/transform_token_to_name.py", "language": "python", "type": "transformer", "uuid": "transform_token_to_name"}, "transformers/fill_na.py:transformer:python:fill na": {"content": "from mage_ai.data_cleaner.transformer_actions.constants import ImputationStrategy\nfrom mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pyspark.sql import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Execute Transformer Action: ActionType.IMPUTE\n\n    Docs: https://docs.mage.ai/guides/transformer-blocks#fill-in-missing-values\n    \"\"\"\n\n    columns_to_impute = ['low', 'high', 'open', 'close', 'volume']\n    \n    action = build_transformer_action(\n        df,\n        action_type=ActionType.IMPUTE,\n        arguments=columns_to_impute,  # Specify column to impute\n        axis=Axis.COLUMN,\n        options={'strategy': ImputationStrategy.MEDIAN},  # Specify imputation strategy\n    )\n\n    df = BaseAction(action).execute(df)\n\n    df = df.dropna(subset=['token', 'date'])\n\n    return BaseAction(action).execute(df)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/fill_na.py", "language": "python", "type": "transformer", "uuid": "fill_na"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}